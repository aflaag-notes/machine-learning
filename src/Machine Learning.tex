\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%%

\def\useItalian{0}  % 1 = Italian, 0 = English

\def\courseName{Machine Learning}

\def\coursePrerequisites{
    TODO
}

\def\book{TODO}

% \def\authorName{Simone Bianco}
% \def\email{bianco.simone@outlook.it}
% \def\github{https://github.com/Exyss/university-notes}
% \def\linkedin{https://www.linkedin.com/in/simone-bianco}

\def\authorName{Alessio Bandiera}
\def\email{alessio.bandiera02@gmail.com}
\def\github{https://github.com/aflaag-notes}
\def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

% Do not change

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../packages/Nyx/nyx-packages}
\usepackage{../../packages/Nyx/nyx-styles}
\usepackage{../../packages/Nyx/nyx-frames}
\usepackage{../../packages/Nyx/nyx-macros}
\usepackage{../../packages/Nyx/nyx-title}
\usepackage{../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../packages/Nyx/logo.png}

\ifx\useItalian0
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} Universit√† di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \subtitle{Appunti integrati con il libro \book}
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \subtitle{Lecture notes integrated with the book \book}
    \author{\textit{Author}\\\authorName}
\fi

\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%
    
    \chapter{TODO}
    
    \section{Learning problems}

    A \tbf{learning problem} is defined by the following \tit{three components}.

    \begin{frameddefn}{Learning}
        \tbf{Learning} is defined as \tit{improving}, through \tit{experience} $E$, at some \tit{task} $T$, with respect to a \tit{performance measure} $P$.
    \end{frameddefn}

    \begin{example}[Machine Learning problem]
        Consider the problem of learning how to play \href{https://en.wikipedia.org/wiki/Checkers}{Checkers}; in this example, the \tit{task} $T$ is to be able to play the game itself, the \tit{performance measure} $P$ could be the percentage of games won in a tournament, but \tit{experience} $E$ is more complex.
    \end{example}

    In general, \tit{experience} can be acquired in several ways:

    \begin{itemize}
        \item in this example, a human expert may suggest optimal moves for each configuration of the board; however, this approach may not generalize for any problem, as human experts may not exists for certain tasks;
        \item alternatively, the computer may play against a human, and automatically detect win, draw and loss configurations;
        \item lastly, the computer may play against itself, learning from its own successes and failures.
    \end{itemize}

    For this particular game, a possible \tbf{target function} (the function that would be useful to learn in order to solve the learning problem) could be the following $$\func{\mathrm{ChooseMove}}{\mathrm{Board}}{\mathrm{Move}}$$ which, given a board state, returns the best move to perform, but also $$\func{V}{\mathrm{Board}}{\R}$$ which assigns a \tit{score} to a given board.

    For instance, consider the following target function: $$V(b) = w_0 + w_1 \cdot bp(b) + w_2 \cdot rp(b) + w_3 \cdot bk(b) + w_4 \cdot rk(b) + w_5 \cdot bt(b) + w_6 \cdot rt(b)$$ where $b$ is a given \tit{board state}, and

    \begin{itemize}
        \item $bp(b)$ is the number of \tit{black pieces}
        \item $rp(b)$ is the number of \tit{red pieces}
        \item $bk(b)$ is the number of \tit{black kings}
        \item $rk(b)$ is the number of \tit{red kings}
        \item $bt(b)$ is the number of \tit{red pieces threatened by black pieces}
        \item $rt(b)$ is the number of \tit{black pieces threatened by red pieces}
    \end{itemize}

    In this formulation, $V$ is a \tit{linear compbination} of multiple coefficients $w_i$, which are unknown. Therefore, in this example \tbf{goal} of the \tit{learning problem} is to \tbf{learn $V$}, or equivalently, to \tbf{estimate each coefficient $w_i$}. Note that this function \tit{can be computed}.
    
    \begin{frameddefn}{Dataset}
        Let $V(b)$ be the \tit{true target function} (always \tit{unknown}), $\hat V(b)$ be the \tit{learned function} --- an approximation of $V(b)$ computed by the \tit{learning algorithm} --- and $V_t(b)$ the \tit{training value} of $b$ in the \tit{training data}. Lastly, let $X$ be an input domain.

        Given a set of $n$ inputs $$X_D := \{b_i \mid i \in [1, n]\} \subset X$$ a \tbf{dataset} is a set of \tit{samples}, and it is denoted as $$D = \{(b_i, V_t(b_i)) \mid b_i \in X_D\}$$
    \end{frameddefn}

    In the previous example, $\hat V(b)$ would have the following form $$\hat V(b) = \hat w_0 + \hat w_1 \cdot bp(b) + \hat w_2 \cdot rp(b) + \hat w_3 \cdot bk(b) + \hat w_4 \cdot rk(b) + \hat w_5 \cdot bt(b) + \hat w_6 \cdot rt(b)$$

    \begin{frameddefn}{Machine learning problem}
        A \tbf{machine learning problem} is the \tit{task} of \tit{learning a function} $\func{f}{X}{Y}$, given a \tit{dataset} $D$.

        To \tbf{learn a function} $f$ means \tit{computing an approximation function} $\hat f$ that returns values as close as possible to $f$, especialy for values \tit{outside $D$} $$\forall x \in  X - X_D \quad \hat f(x) \approx f(x)$$
    \end{frameddefn}

    Note that $\abs{X_D} << \abs{X}$, which makes the task of learning $f$ quite challenging.

    There are multiple types of Machine Learning (ML) problems, such as \tit{dataset type} and \tit{target function type}. The various ML problems will be discussed in later sections.

    \begin{frameddefn}{Hypothesis space}
        Given an ML problem, an \tbf{hypothesis} $h$ for the problem is an approximation of its target function, and its \tbf{hypothesis speace} $H$ is the set of all possible hypothesis, i.e. the set of all functions that can be learned, which correspond to all the approximations of the target function of the ML problem.
    \end{frameddefn}

    Given this definition, \tbf{learning} can be defined as \tit{searching in the hypothesis space}, using the dataset $D$ and some performance function $P$ of the given ML problem $$h^* = \argmax_{h \in H}{P(h, D)}$$ A \tbf{performance measure} is a metric that evaluates the correctness of a given hypothesis, by comparing $h(x)$ and $f(x)$ for all $x \in X_D$, where $f$ is the target function of the ML problem.

    \begin{example}[Hypothesis]
        Consider the ML problem of \tit{classifying natural numbers into primes and composite numbers}. The \tit{target function} would be the following $$\func{f}{\N}{\{\Primes, \N - \Primes\}}$$ A dataset $D$ for this ML problem would look like the following example $$D = \{(1, \Primes), (3, \Primes), (5, \Primes), (6, \N - \Primes), (8, \N - \Primes), (10, \N - \Primes)\}$$ The hypothesis space is the set of all possible \tit{classification functions} of the form $$\func{h_A}{\N}{\{A, \N - A\}}$$
    \end{example}

    placeholder \todo{repr vs gener??}

    \begin{frameddefn}{Hypothesis consistency}
        Given an ML problem defined by a target function $\func{c}{X}{Y}$ --- for some sets $X$ and $Y$ --- and a training dataset $D = \{(x, c(x))\}$, an hypothesis $h \in H$ is said to be \tbf{consistent with $D$} if and only if $$\forall x \in D \quad h(x) = c(x)$$
    \end{frameddefn}

    Note that this definition is important, because $h(x)$ can be evaluated for any $x \in X$, but only inputs that appear in the dataset can be verified, for which $c(x)$ is known. Therefore, \tit{consistency} should be desirable for an hypothesis, since the real goal of an ML system is to find \tit{the best} $h$ that predicts correct values of $h(x')$, for instances $x' \notin X_D$, with respect to the unknown values $c(x')$.

    \begin{frameddefn}{Inductive learning hypothesis}
        The \tbf{inductive learning hypothesis} states the following: \displayquote{Given an ML problem, any hypothesis that approximates the target function well over a sufficiently large set of training examples, will also approximate the target function well over other unobserved examples.}
    \end{frameddefn}

    \begin{frameddefn}{Version space}
        The \tbf{version space} $VS_{H, D}$ of an ML problem, is the subset of hypotheses of $H$ consistent with all training examples in $D$. Using symbols $$VS_{H, D}: = \{h \in H \mid \forall x \in X_D \quad h(x) = c(x)\} \subset H$$
    \end{frameddefn}

    \begin{framedalgo}{List-Then-Eliminate}
        Given an ML problem, the algorithm returns $VS_{H, D}$. \\

        \hrule

        \quad
        \label{alg:lte}
        \begin{algorithmic}[1]
            \Function{listThenEliminate}{$X_D$, $D$}
                \State $VS_{H, D} := H$ \Comment{initially it contains any hypothesis}
                \For{$(x, c(x)) \in D$}
                    \State $H' := \{h \in H \mid h(x) \neq c(x)\}$ \Comment{set of inconsistent hypotheses for $x$}
                    \State $VS_{H, D} = VS_{H, D} - H'$
                \EndFor
                \State \tbf{return} $VS_{H,D}$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    This algorithm can theoretically find the version space for any ML problem, but \tit{it is not computable}, as it requires to \tbf{enumerate all the possible hypotheses}.

\end{document}
